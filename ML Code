import pymysql
import pandas as pd

# Connect to MySQL database
conn = pymysql.connect(
    host="localhost",
    user="*****",
    password="************",
    database="mydatabase"
)

# SQL Query to extract features and poverty rate
query = """
SELECT 
    c.state, 
    c.Year,
    c.legal_corruption, 
    c.illegal_corruption, 
    p.poverty_rate,
    cl.Housing,
    cl.Health,
    e.average_education_l,
    d.rate as drug_death_rate
FROM corruption c
JOIN poverty p ON c.state = p.state AND c.Year
JOIN cost_of_living cl ON c.state = cl.state
JOIN education e on c.state = e.state
JOIN drug_use d ON  c.state = d.state AND c.year = d.year
ORDER BY p.poverty_rate desc;
"""

# Load data into a Pandas DataFrame
df = pd.read_sql(query, conn)

# Close database connection
conn.close()

# Show first few rows
df.head()


# Drop non-numeric columns (and columns that will mess with model)
df_clean = df.drop(columns=["state", "Year"])

# Check for missing values
df_clean = df_clean.dropna()  # Drop rows with missing values

# Define Features (X) and Target (y)
X = df_clean.drop(columns=["poverty_rate"])  # Features
y = df_clean["poverty_rate"]  # Target variable (poverty)

# Display data shape
print("Features shape:", X.shape)
print("Target shape:", y.shape)


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.preprocessing import StandardScaler

# Scale the data (Neural Networks perform better with normalized data)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the Neural Network Model
model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # First hidden layer
    Dense(32, activation='relu'),  # Second hidden layer
    Dense(16, activation='relu'),  # Third hidden layer
    Dense(1, activation='linear')  # Output layer for regression
])

# Compile the Model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the Model
history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=16, validation_data=(X_test_scaled, y_test), verbose=1)

# Evaluate Model
test_loss, test_mae = model.evaluate(X_test_scaled, y_test)
print("Test MAE:", test_mae)
0.8411

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import keras.backend as K
import numpy as np

# Define RMSE Function for Tracking During Training
def rmse(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_true - y_pred)))

# Compile Model with RMSE
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])  # RMSE removed here

# Train the Model
history = model.fit(X_train_scaled, y_train, epochs=150, batch_size=32,
                    validation_data=(X_test_scaled, y_test),
                    callbacks=[early_stopping], verbose=1)

# Evaluate Model Performance
test_loss, test_mae = model.evaluate(X_test_scaled, y_test)
test_rmse = np.sqrt(test_loss)  # RMSE is sqrt of MSE (which is test_loss)

# Print Results
print(f"Test MAE: {test_mae}")
print(f"Test RMSE: {test_rmse}")  # Now calculated manually
Test MAE: 0.771795392036438
Test RMSE: 0.9938675181160372

import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Train XGBoost model
xgb_model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.05, max_depth=6, random_state=42)
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Calculate RMSE
rmse_xgb = mean_squared_error(y_test, y_pred_xgb, squared=False)

# Calculate MAE
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)

# Print Evaluation Metrics
print(f"✅ XGBoost RMSE: {rmse_xgb:.4f}")
print(f"✅ XGBoost MAE: {mae_xgb:.4f}")

# Feature Importance
import matplotlib.pyplot as plt

xgb_importance = xgb_model.feature_importances_
sorted_indices = xgb_importance.argsort()[::-1]

plt.figure(figsize=(10,6))
plt.barh(X.columns[sorted_indices], xgb_importance[sorted_indices], color="skyblue")
plt.xlabel("Feature Importance Score")
plt.ylabel("Feature")
plt.title("XGBoost Feature Importance")
plt.gca().invert_yaxis()
plt.show()

XGBoost RMSE: 0.7303
✅ XGBoost MAE: 0.4747

